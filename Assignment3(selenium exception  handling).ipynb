{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccd7f22c",
   "metadata": {},
   "source": [
    "  # Assignment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09f60f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: selenium in c:\\users\\dell\\appdata\\roaming\\python\\python39\\site-packages (4.7.2)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\dell\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\dell\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (2021.10.8)\n",
      "Requirement already satisfied: outcome in c:\\users\\dell\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\dell\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.0.4)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\dell\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: sortedcontainers in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.0)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\dell\\appdata\\roaming\\python\\python39\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\dell\\appdata\\roaming\\python\\python39\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    " # Let's first install the selenium library\n",
    "! pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3671da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now import all the required libraries.\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f4ad1fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first connect to the web driver\n",
    "driver = webdriver.Chrome(r\"c:\\Users\\dell\\Downloads\\chromedriver_win32\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b3a245",
   "metadata": {},
   "source": [
    "# 1. Write a python program which searches all the product under a particular product from www.amazon.in.The product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for guitars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d1c444",
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://www.amazon.in/\"\n",
    "driver.get(url)\n",
    "\n",
    "time.sleep(2)\n",
    "search_bar=driver.find_element(By.XPATH,\"//input[@type='text']\")\n",
    "search_bar\n",
    "\n",
    "search_bar.send_keys('guitar')\n",
    "\n",
    "time.sleep(2)\n",
    "# locating the button and clicking it to search for shoes\n",
    "button=driver.find_element(By.XPATH,\"//input[@id='nav-search-submit-button']\")\n",
    "button.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a87ff0b",
   "metadata": {},
   "source": [
    "# 2. In the above question, now scrape the following details of each product listed in first 3 pages of your search results and save it in a dataframe and csv. In case if any product has less than 3 pages in search results then scrape all the products available under that product vertical. Details to be scraped are: \"Brand Name\", \"Name of the Product\", \"Rating\", \"No. of Ratings\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\", \"Other Details\" and “Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9120a2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty list\n",
    "brand=[]\n",
    "name_pr=[]\n",
    "rating=[]\n",
    "no_rating=[]\n",
    "price=[]\n",
    "re_ex=[]\n",
    "exp_del=[]\n",
    "avail=[]\n",
    "other_detail=[]\n",
    "\n",
    "\n",
    "for page in range(0,3):\n",
    "    \n",
    "    brands=driver.find_elements_by_xpath(\"//span[@class='a-size-base-plus a-color-base']\")#scraping brands name by class name='_2WkVRV'\n",
    "    for i in brands:\n",
    "        brand.append(i.text)#appending the text in Brand list\n",
    "  \n",
    "           \n",
    "    name_product=driver.find_elements_by_xpath('//span[@class=\"a-size-base-plus a-color-base a-text-normal\"]')#scraping name_product from the xpath\n",
    "    for j in name_product:\n",
    "        name_pr.append(j.text)# appending the text in name_pr\n",
    "        \n",
    "    prices=driver.find_elements_by_xpath('//span[@class=\"a-price-whole\"]') \n",
    "    for k in prices:\n",
    "        price.append(k.text)# appending the text in price list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf53d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(brand),len(price),len(name_pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3468956f",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_urls=[]\n",
    "for page in range(0,3):\n",
    "    url1=driver.find_elements_by_xpath(\"//a[@class='a-link-normal a-text-normal']\")\n",
    "    for t in url1:\n",
    "        page_urls.append(t.get_attribute('href'))\n",
    "    page_urls \n",
    "    \n",
    "    time.sleep(3)\n",
    "  #  for scraping no_rating\n",
    "    no_ratings=driver.find_elements_by_xpath('//a[@id=\"acrCustomerReviewLink\"]')\n",
    "    for l in no_ratings:\n",
    "        if l.text is None:\n",
    "            no_rating.append(\"--\")\n",
    "        else:\n",
    "            no_rating.append(l.text)\n",
    "     \n",
    "    time.sleep(3)\n",
    "  # for scraping rating\n",
    "    ratings=driver.find_elements_by_xpath('//span[@data-hook=\"acr-average-stars-rating-text\"]')\n",
    "    for m in ratings:\n",
    "        rating.append(m.text)\n",
    "        \n",
    "    time.sleep(2)   \n",
    "  # for scraping return/exchange   \n",
    "    return_ex= driver.find_elements_by_xpath('//a[@class=\"a-size-small a-link-normal a-text-normal\"]')\n",
    "    for n in return_ex:\n",
    "        re_ex.append(n.text)\n",
    "        \n",
    "    time.sleep(2)\n",
    "  # for scraping expected delivery\n",
    "    expec_del=driver.find_elements_by_xpath('//div[@id=\"ddmDeliveryMessage\"]')\n",
    "    for o in expec_del:\n",
    "        exp_del.append(o.text)\n",
    "        \n",
    "    time.sleep(2)   \n",
    "  # for scraing other detail\n",
    "    pr_detail=driver.find_elements_by_xpath('//hr[@aria-hidden=\"true\"]')\n",
    "    for p in pr_detail:\n",
    "        other_detail.append(p.text)\n",
    "        \n",
    "    time.sleep(2)    \n",
    "  # for scraping availability\n",
    "    pr_avail=driver.find_elements_by_xpath('//div[@id=\"availability\"]')\n",
    "    for q in pr_avail:\n",
    "        avail.append(q.text)\n",
    "        \n",
    "    time.sleep(2)    \n",
    "  # for scraping product url\n",
    " #   product_url=driver.find_elements_by_xpath('//a[@class=\"a-link-normal a-text-normal\"]')\n",
    "   # for r in product_url:\n",
    "    #    pr_url.append(r.text)\n",
    "\n",
    "    \n",
    "    \n",
    "    len(no_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25091cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combining all lists in to a single dataframe\n",
    "df_shoes=pd.DataFrame({'Brand_name':brand,\n",
    "                      'Product_name':name_pr,\n",
    "                      'Ratings':rating,\n",
    "                      'No_ratings':no_rating,\n",
    "                      'Price':price,\n",
    "                      'Return/Exchange':re_ex,\n",
    "                      'Expected_del':exp_del,\n",
    "                      'Availability':avail,\n",
    "                      'Other_detail':other_detail,\n",
    "                      'Product_URL':pr_url})\n",
    "df_shoes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1661230",
   "metadata": {},
   "source": [
    "# 3. Write a python program to access the search bar and search button on images.google.com and scrape 10images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4da495",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.google.com\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41684bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding element for job search bar\n",
    "search_ph = driver.find_element(By.XPATH,\"//input[@type='text']\")\n",
    "search_ph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb312842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding element for job search bar\n",
    "search_ph = driver.find_element(By.XPATH,\"//input[@type='text']\")\n",
    "search_ph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1913083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write on search bar\n",
    "search_ph.send_keys(\"car images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faea1a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking the search button\n",
    "search_button=driver.find_element(By.XPATH,\"//input[@class='gLFyf']\")\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7552e6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(20):\n",
    "    driver.execute_script(\"window.scrollBy(0,1000)\")\n",
    "    \n",
    "images = driver.find_element(By.XPATH,\"//img[@class='rg_i Q4LuWd']\")\n",
    "\n",
    "img_urls =[]\n",
    "img_data =[]\n",
    "for image in images:\n",
    "    source= image.get_attribute('scr')\n",
    "    if source is not None:\n",
    "        if(source[0:4]=='http'):\n",
    "            img_urls.append(source)\n",
    "            \n",
    "            \n",
    "for i in range(len(img_urls)):\n",
    "    if i>10:\n",
    "        breakBy.XPATH,\n",
    "    print(\"Downloading {0} of {1} images\" .format(i,10))\n",
    "    response=requests.get(img_urls[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94282c65",
   "metadata": {},
   "source": [
    "# 4.Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”, “Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the details is missing then replace it by “- “. Save your results in a dataframe and CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e549c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.flipkart.com\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d92cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding element for job search bar\n",
    "search_ph = driver.find_element(By.XPATH,\"//input[@type='text']\")\n",
    "search_ph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a3cfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write on search bar\n",
    "search_ph.send_keys(\"Smartphone \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ff8a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking the search button\n",
    "search_button=driver.find_element(By.XPATH,\"//button[@class='L0Z3Pu']\")\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5414b1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty list\n",
    "brand=[]\n",
    "pro_detail=[]\n",
    "\n",
    "for page in range(0,3):\n",
    "    \n",
    "    brand=driver.find_element(By.XPATH,\"//div[@class='_4rR01T']\")#scraping brands name\n",
    "    for i in brand:\n",
    "        brand.append(i.text)#appending the text in Brand list\n",
    "  \n",
    "           \n",
    "    detail_product=driver.find_element(By.XPATH,\"//div[@class='fMghEO']\")#scraping name detailed\n",
    "    for j in detail_product:\n",
    "        pro_detail.append(j.text)# appending the text in name_pr\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106af26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(brand),len(pro_detail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62465317",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_urls=[]\n",
    "for page in range(0,3):\n",
    "    url1=driver.find_element(By.XPATH,\"//a[@class='_1fQZEK']\")\n",
    "    for t in url1:\n",
    "        page_urls.append(t.get_attribute('href'))\n",
    "    page_urls \n",
    "    \n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0b13ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combining all lists in to a single dataframe\n",
    "df_phone=pd.DataFrame({'Brand_name':brand,\n",
    "                      'pr_detail':pro_detail,\n",
    "                      'Product_URL':page_urls})\n",
    "df_phone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa42351",
   "metadata": {},
   "source": [
    "# 5.Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5742fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required modules\n",
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "  \n",
    "  \n",
    "# assign url in the webdriver object\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.google.co.in/maps/@10.8091781,78.2885026,7z\")\n",
    "sleep(2)\n",
    "  \n",
    "  \n",
    "# search locations\n",
    "def searchplace():\n",
    "    Place = driver.find_element_by_class_name(\"tactile-searchbox-input\")\n",
    "    Place.send_keys(\"Tiruchirappalli\")\n",
    "    Submit = driver.find_element(By.XPATH,\n",
    "        \"/html/body/jsl/div[3]/div[9]/div[3]/div[1]/div[1]/div[1]/div[2]/div[1]/button\")\n",
    "    Submit.click()\n",
    "    \n",
    "searchplace()\n",
    "  \n",
    "  \n",
    "# get directions\n",
    "def directions():\n",
    "    sleep(10)\n",
    "    directions = driver.find_element(By.XPATH,\n",
    "        \"/html/body/jsl/div[3]/div[9]/div[7]/div/div[1]/div/div/div[5]/div[1]/div/button\")\n",
    "    directions.click()\n",
    "    \n",
    "directions()\n",
    "  \n",
    "\n",
    "  \n",
    "  \n",
    "# find place\n",
    "def find():\n",
    "    sleep(6)\n",
    "    find = driver.find_element(By.XPATH,\n",
    "        \"/html/body/jsl/div[3]/div[9]/div[3]/div[1]/div[2]/div/div[3]/div[1]/div[1]/div[2]/div/div/input\")\n",
    "    find.send_keys(\"Tirunelveli\")\n",
    "    sleep(2)\n",
    "    search = driver.find_element(By.XPATH,\n",
    "        \"/html/body/jsl/div[3]/div[9]/div[3]/div[1]/div[2]/div/div[3]/div[1]/div[1]/div[2]/button[1]\")\n",
    "    search.click()\n",
    "    \n",
    "find()\n",
    "  \n",
    "  \n",
    "# get transportation details\n",
    "def kilometers():\n",
    "    sleep(5)\n",
    "    Totalkilometers = driver.find_element(By.XPATH,\n",
    "        \"/html/body/jsl/div[3]/div[9]/div[7]/div/div[1]/div/div/div[5]/div[1]/div[1]/div[1]/div[1]/div[2]/div\")\n",
    "    print(\"Total Kilometers:\", Totalkilometers.text)\n",
    "    sleep(5)\n",
    "    Bus = driver.find_element(By.XPATH,\n",
    "        \"/html/body/jsl/div[3]/div[9]/div[7]/div/div[1]/div/div/div[5]/div[1]/div[1]/div[1]/div[1]/div[1]/span[1]\")\n",
    "    print(\"Bus Travel:\", Bus.text)\n",
    "    sleep(7)\n",
    "    Train = driver.find_element(By.XPATH\n",
    "        \"/html/body/jsl/div[3]/div[9]/div[7]/div/div[1]/div/div/div[5]/div[2]/div[1]/div[2]/div[1]/div\")\n",
    "    print(\"Train Travel:\", Train.text)\n",
    "    sleep(7)\n",
    "    \n",
    "kilometers()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b16129",
   "metadata": {},
   "source": [
    "# 6. Write a program to scrap details of all the funding deals for second quarter (i.e Jan 21 – March 21)from trak.in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c6706a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0a5b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_url = \"https://trak.in/india-startup-funding-investment-2015/\"\n",
    "html_response = requests.get(start_url)\n",
    "html_response.status_code # code 200 means connecition was sucessful and it is now active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca11bba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html_response.content, 'html.parser')\n",
    "#print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308863bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "more_data_urls = [start_url]\n",
    "\n",
    "for h3_tag in soup.find_all(name=\"h3\"):\n",
    "    more_data_urls.append(h3_tag.find(name='a').get('href'))\n",
    "\n",
    "more_data_urls[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1832aa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_row_list = []\n",
    "column_name = ['Sr. No.', 'Date (dd/mm/yyyy)', 'Startup Name', 'Industry/ Vertical', 'Sub-Vertical', 'City / Location', 'Investors’ Name', 'Investment Type', 'Amount (in USD)']\n",
    "more_data_urls = set(more_data_urls)\n",
    "\n",
    "urls_count = 1\n",
    "for url in more_data_urls:\n",
    "    html_response = requests.get(url)\n",
    "    html_response.status_code\n",
    "    soup = BeautifulSoup(html_response.content, 'html.parser')\n",
    "    \n",
    "    class_list = []\n",
    "    for element in soup.find_all(class_=True):\n",
    "        class_list.extend(element[\"class\"])\n",
    "    class_list = [cls for cls in class_list if 'tablepress-id-' in cls] \n",
    "    \n",
    "    if len(class_list) < 1:\n",
    "        skip_first_row = True\n",
    "        class_list.append(None)\n",
    "        for class_ in class_list:\n",
    "            tbl=soup.find(name='table') #, class_=class_)\n",
    "\n",
    "            n_rows = 0\n",
    "            for tr in tbl.find_all('tr'):\n",
    "                if skip_first_row == True:\n",
    "                    skip_first_row = False\n",
    "                    continue\n",
    "                new_row = {}\n",
    "                for col_id, td in enumerate(tr.find_all('td')):\n",
    "                    if col_id < len(column_name):\n",
    "                        new_row[column_name[col_id]] = td.text\n",
    "                if not new_row == {}:\n",
    "                    n_rows += 1\n",
    "                    new_row_list.append(new_row)\n",
    "            #print(\"class_list-old:\", class_, len(new_row_list), n_rows, url)\n",
    "        else:\n",
    "            for class_ in class_list:\n",
    "                tbl=soup.find(name='table', class_=class_)\n",
    "\n",
    "            n_rows = 0\n",
    "            for tr in tbl.find_all('tr'):\n",
    "                new_row = {}\n",
    "                for col_id, td in enumerate(tr.find_all('td')):\n",
    "                    if col_id < len(column_name):\n",
    "                        new_row[column_name[col_id]] = td.text\n",
    "                if not new_row == {}:\n",
    "                    n_rows += 1\n",
    "                    new_row_list.append(new_row)\n",
    "            #print(\"class_list-new :\", class_, len(new_row_list), n_rows, url)\n",
    "\n",
    "data = pd.DataFrame(new_row_list, columns=column_name)\n",
    "print(\"Data shape :\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f336a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77330a9",
   "metadata": {},
   "source": [
    "# 7.Write a program to scrap all the available details of best gaming laptops from digit.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90a1303",
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://www.digit.in/top-products/best-gaming-laptops-40.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7e6a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aa787c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Brands=[]\n",
    "Products_Description=[]\n",
    "Specification=[]\n",
    "Price=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad091377",
   "metadata": {},
   "outputs": [],
   "source": [
    "br=driver.find_elements(By.XPATH,\"//div[@class='TopNumbeHeading active sticky-footer']\")\n",
    "len(br)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f158e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in br:\n",
    "   \n",
    "    Brands.append(str(i.text).replace(\"\\n\",\"\"))\n",
    "Brands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbbfbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp=driver.find_elements(By.XPATH,\"//div[@class='Specs-Wrap']\")\n",
    "len(sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d92e6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sp:\n",
    "   \n",
    "    Specification.append(str(i.text).replace(\"\\n\",\"\"))\n",
    "Specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3a759d",
   "metadata": {},
   "outputs": [],
   "source": [
    "des=driver.find_elements(By.XPATH,\"//div[@class='Section-center']\")\n",
    "len(des)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d352f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in des:\n",
    "   \n",
    "    Products_Description.append(str(i.text).replace(\"\\n\",\"\"))\n",
    "Products_Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c0c551",
   "metadata": {},
   "outputs": [],
   "source": [
    "pri=driver.find_elements(By.XPATH,\"//td[@class='smprice']\")\n",
    "len(pri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c91b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in pri:\n",
    "   \n",
    "    Price.append(str(i.text).replace(\"\\n\",\"\"))\n",
    "Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b82914",
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_lap=pd.DataFrame([])\n",
    "digit_lap['Brands']=Brands[0:10]\n",
    "digit_lap['Price']=Price[0:10]\n",
    "digit_lap['Specification']=Specification[0:10]\n",
    "digit_lap['Description']=Products_Description[0:10]\n",
    "digit_lap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f61e513",
   "metadata": {},
   "source": [
    "# 8.Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped: “Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f083574",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.forbes.com\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fd655d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"97febb26cd876cded146eddcef79465a\", element=\"8d66c10a-1bbd-4ecb-9998-eaadbac30b1d\")>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding element for billionaires search bar\n",
    "search_bill = driver.find_element(By.XPATH,\"//button[@class='icon--search']\")\n",
    "search_bill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08f5807b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write on search bar\n",
    "search_bill.send_keys(\"all billionaires\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "605de827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking the search button\n",
    "search_button=driver.find_element(By.XPATH,\"//button[@class='search-modal__submit']\")\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3124c095",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_urls=[]\n",
    "for page in range(0,3):\n",
    "    url1=driver.find_element(By.XPATH,\"//a[@class='stream-item__title']\")\n",
    "    for t in url1:\n",
    "        page_urls.append(t.get_attribute('href'))\n",
    "        page_urls \n",
    "    \n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e554c577",
   "metadata": {},
   "source": [
    "# 9.Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted from any YouTube Video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca13e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "\n",
    "def ScrapComment(url):\n",
    "    option = webdriver.FirefoxOptions()\n",
    "    option.add_argument(\"--headless\")\n",
    "    driver = webdriver.Firefox(executable_path=GeckoDriverManager().install(), options=option)\n",
    "    driver.get(url)\n",
    "    prev_h = 0\n",
    "    while True:\n",
    "        height = driver.execute_script(\"\"\"\n",
    "                function getActualHeight() {\n",
    "                    return Math.max(\n",
    "                        Math.max(document.body.scrollHeight, document.documentElement.scrollHeight),\n",
    "                        Math.max(document.body.offsetHeight, document.documentElement.offsetHeight),\n",
    "                        Math.max(document.body.clientHeight, document.documentElement.clientHeight)\n",
    "                    );\n",
    "                }\n",
    "                return getActualHeight();\n",
    "            \"\"\")\n",
    "        driver.execute_script(f\"window.scrollTo({prev_h},{prev_h + 200})\")\n",
    "        # fix the time sleep value according to your network connection\n",
    "        time.sleep(1)\n",
    "        prev_h +=200  \n",
    "        if prev_h >= height:\n",
    "            break\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    driver.quit()\n",
    "    title_text_div = soup.select_one('#container h1')\n",
    "    title = title_text_div and title_text_div.text\n",
    "    comment_div = soup.select(\"#content #content-text\")\n",
    "    comment_list = [x.text for x in comment_div]\n",
    "    print(title, comment_list)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    urls = [\n",
    "        \"https://www.youtube.com/watch?v=cgNQgcUgq0U\",\n",
    "        \"https://www.youtube.com/watch?v=MkE_EwO76b0\",\n",
    "        'https://www.youtube.com/watch?v=XVv6mJpFOb0',\n",
    "    ]\n",
    "    ScrapComment(urls[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665caf60",
   "metadata": {},
   "source": [
    "# 10.Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in“London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews,overall reviews, privates from price, dorms from price, facilities and property description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "427f8469",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.hostelworld.com\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc257a6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"a78963d50bcb66a03de241a87ecedf7d\", element=\"ed34a2ad-1b68-4476-8756-e06ddea649b1\")>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding element for billionaires search bar\n",
    "search_hostel = driver.find_element(By.XPATH,\"//input[@class='location-text']\")\n",
    "search_hostel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c2d5dcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write on search bar\n",
    "search_hostel.send_keys(\"London\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8eacd876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking the search button\n",
    "search_hostel=driver.find_element(By.XPATH,\"//div[@class='search-button']\")\n",
    "search_hostel.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa634e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_urls=[]\n",
    "for page in range(0,3):\n",
    "    url1=driver.find_element(By.XPATH,\"//div[@class='property-card']\")\n",
    "    for t in url1:\n",
    "        page_urls.append(t.get_attribute('href'))\n",
    "        page_urls \n",
    "    \n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b6471d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
